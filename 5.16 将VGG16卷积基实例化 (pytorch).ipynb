{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import AverageMeter, ProgressMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 150, 150]           1,792\n",
      "              ReLU-2         [-1, 64, 150, 150]               0\n",
      "            Conv2d-3         [-1, 64, 150, 150]          36,928\n",
      "              ReLU-4         [-1, 64, 150, 150]               0\n",
      "         MaxPool2d-5           [-1, 64, 75, 75]               0\n",
      "            Conv2d-6          [-1, 128, 75, 75]          73,856\n",
      "              ReLU-7          [-1, 128, 75, 75]               0\n",
      "            Conv2d-8          [-1, 128, 75, 75]         147,584\n",
      "              ReLU-9          [-1, 128, 75, 75]               0\n",
      "        MaxPool2d-10          [-1, 128, 37, 37]               0\n",
      "           Conv2d-11          [-1, 256, 37, 37]         295,168\n",
      "             ReLU-12          [-1, 256, 37, 37]               0\n",
      "           Conv2d-13          [-1, 256, 37, 37]         590,080\n",
      "             ReLU-14          [-1, 256, 37, 37]               0\n",
      "           Conv2d-15          [-1, 256, 37, 37]         590,080\n",
      "             ReLU-16          [-1, 256, 37, 37]               0\n",
      "        MaxPool2d-17          [-1, 256, 18, 18]               0\n",
      "           Conv2d-18          [-1, 512, 18, 18]       1,180,160\n",
      "             ReLU-19          [-1, 512, 18, 18]               0\n",
      "           Conv2d-20          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-21          [-1, 512, 18, 18]               0\n",
      "           Conv2d-22          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-23          [-1, 512, 18, 18]               0\n",
      "        MaxPool2d-24            [-1, 512, 9, 9]               0\n",
      "           Conv2d-25            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-26            [-1, 512, 9, 9]               0\n",
      "           Conv2d-27            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-28            [-1, 512, 9, 9]               0\n",
      "           Conv2d-29            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-30            [-1, 512, 9, 9]               0\n",
      "        MaxPool2d-31            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 96.93\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 624.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 150, 150), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 150, 150]           1,792\n",
      "              ReLU-2         [-1, 64, 150, 150]               0\n",
      "            Conv2d-3         [-1, 64, 150, 150]          36,928\n",
      "              ReLU-4         [-1, 64, 150, 150]               0\n",
      "         MaxPool2d-5           [-1, 64, 75, 75]               0\n",
      "            Conv2d-6          [-1, 128, 75, 75]          73,856\n",
      "              ReLU-7          [-1, 128, 75, 75]               0\n",
      "            Conv2d-8          [-1, 128, 75, 75]         147,584\n",
      "              ReLU-9          [-1, 128, 75, 75]               0\n",
      "        MaxPool2d-10          [-1, 128, 37, 37]               0\n",
      "           Conv2d-11          [-1, 256, 37, 37]         295,168\n",
      "             ReLU-12          [-1, 256, 37, 37]               0\n",
      "           Conv2d-13          [-1, 256, 37, 37]         590,080\n",
      "             ReLU-14          [-1, 256, 37, 37]               0\n",
      "           Conv2d-15          [-1, 256, 37, 37]         590,080\n",
      "             ReLU-16          [-1, 256, 37, 37]               0\n",
      "        MaxPool2d-17          [-1, 256, 18, 18]               0\n",
      "           Conv2d-18          [-1, 512, 18, 18]       1,180,160\n",
      "             ReLU-19          [-1, 512, 18, 18]               0\n",
      "           Conv2d-20          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-21          [-1, 512, 18, 18]               0\n",
      "           Conv2d-22          [-1, 512, 18, 18]       2,359,808\n",
      "             ReLU-23          [-1, 512, 18, 18]               0\n",
      "        MaxPool2d-24            [-1, 512, 9, 9]               0\n",
      "           Conv2d-25            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-26            [-1, 512, 9, 9]               0\n",
      "           Conv2d-27            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-28            [-1, 512, 9, 9]               0\n",
      "           Conv2d-29            [-1, 512, 9, 9]       2,359,808\n",
      "             ReLU-30            [-1, 512, 9, 9]               0\n",
      "        MaxPool2d-31            [-1, 512, 4, 4]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 96.55\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 152.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"/data/bitt/wzq/wzq/python-deep-learning/data/cats_and_dogs_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((150, 150)),\n",
    "                                      transforms.ToTensor(),])\n",
    "val_transform = transforms.Compose([transforms.Resize((150, 150)),\n",
    "                                    transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.ImageFolder(train_dir, train_transform)\n",
    "val_data = data.ImageFolder(validation_dir, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=20, shuffle=True, num_workers=20)\n",
    "val_dataloader = DataLoader(val_data, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloader, sample_count):\n",
    "    with torch.no_grad():\n",
    "        features = torch.zeros(sample_count, 512, 4, 4)\n",
    "        labels = torch.zeros(sample_count)\n",
    "        for i, (input, target) in tqdm(enumerate(dataloader)):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            feature = model(input)\n",
    "            features[i*20: (i+1) * 20, :, :, :] = feature\n",
    "            labels[i*20: (i+1) * 20] = target\n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:03, 25.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = extract_features(train_dataloader, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 512, 4, 4])\n",
      "torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:07,  6.93it/s]\n"
     ]
    }
   ],
   "source": [
    "val_features, val_labels = extract_features(val_dataloader, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 512, 4, 4])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(val_features.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(4 * 4 * 512, 256)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.classifier = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]       2,097,408\n",
      "           Dropout-2                  [-1, 256]               0\n",
      "            Linear-3                    [-1, 1]             257\n",
      "================================================================\n",
      "Total params: 2,097,665\n",
      "Trainable params: 2,097,665\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 8.00\n",
      "Estimated Total Size (MB): 8.04\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bitt/wzq/anaconda3/envs/pytorch_gpu/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "criteria = nn.BCELoss()\n",
    "summary(model, input_size=(4 * 4 * 512,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, device, features, labels, criteria):\n",
    "    losses = AverageMeter(\"Loss\", ':.4e')\n",
    "    accuracy = AverageMeter(\"Acc\", \":6.2f\")\n",
    "    nums =  features.size(0)\n",
    "    progress = ProgressMeter(nums, losses, accuracy)\n",
    "    features = features.view(nums, -1)\n",
    "    batch_size = 20\n",
    "    iters = nums / batch_size\n",
    "    for i in range(int(iters)):\n",
    "        input = features[i*batch_size: (i+1)*batch_size, :]\n",
    "        input = input.to(device)\n",
    "        target = labels[i*batch_size: (i+1)*batch_size]\n",
    "        target = target.to(device)\n",
    "        pred = model(input)\n",
    "        output = pred > 0.8\n",
    "        predicts = torch.zeros_like(pred)\n",
    "        predicts[output] = 1.0\n",
    "        correct = target.eq(predicts.view_as(target)).sum().item() /  batch_size\n",
    "        loss = criteria(pred, target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        accuracy.update(correct, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            progress.pr2int(i * 20)\n",
    "    return losses.avg, accuracy.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, features, labels, criteria):\n",
    "    losses = AverageMeter(\"Loss\", ':.4e')\n",
    "    accuracy = AverageMeter(\"Acc\", \":6.2f\")\n",
    "    nums =  features.size(0)\n",
    "    features = features.view(nums, -1)\n",
    "    batch_size = 20\n",
    "    iters = nums / batch_size\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(int(iters)):\n",
    "            input = features[i*batch_size: (i+1)*batch_size, :]\n",
    "            input = input.to(device)\n",
    "            target = labels[i*batch_size: (i+1)*batch_size]\n",
    "            target = target.to(device)\n",
    "            pred = model(input)\n",
    "            output = pred > 0.8\n",
    "            predicts = torch.zeros_like(pred)\n",
    "            predicts[output] = 1.0\n",
    "            correct = target.eq(predicts.view_as(target)).sum().item() /  batch_size\n",
    "            loss = criteria(pred, target)\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            accuracy.update(correct, batch_size)\n",
    "    return losses.avg, accuracy.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bitt/wzq/anaconda3/envs/pytorch_gpu/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/data/bitt/wzq/anaconda3/envs/pytorch_gpu/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "[   0/2000]\tLoss 7.0250e-01 (7.0250e-01)\tAcc   0.80 (  0.80)\n",
      "[ 200/2000]\tLoss 4.8099e-01 (4.9836e-01)\tAcc   0.65 (  0.63)\n",
      "[ 400/2000]\tLoss 3.0152e-01 (4.1853e-01)\tAcc   0.85 (  0.69)\n",
      "[ 600/2000]\tLoss 2.5325e-01 (3.7439e-01)\tAcc   0.80 (  0.74)\n",
      "[ 800/2000]\tLoss 2.4534e-01 (3.4886e-01)\tAcc   0.85 (  0.76)\n",
      "[1000/2000]\tLoss 4.2606e-01 (3.3298e-01)\tAcc   0.70 (  0.77)\n",
      "[1200/2000]\tLoss 2.4586e-01 (3.1212e-01)\tAcc   0.90 (  0.79)\n",
      "[1400/2000]\tLoss 2.4375e-01 (2.9632e-01)\tAcc   0.85 (  0.80)\n",
      "[1600/2000]\tLoss 2.4214e-01 (2.8516e-01)\tAcc   0.90 (  0.80)\n",
      "[1800/2000]\tLoss 2.7256e-01 (2.7488e-01)\tAcc   0.85 (  0.81)\n",
      "Val loss:0.19373230010271072, Val acc:0.861\n",
      "epoch:  1\n",
      "[   0/2000]\tLoss 2.1971e-01 (2.1971e-01)\tAcc   0.90 (  0.90)\n",
      "[ 200/2000]\tLoss 2.9666e-01 (1.7900e-01)\tAcc   0.75 (  0.87)\n",
      "[ 400/2000]\tLoss 1.4547e-01 (1.7318e-01)\tAcc   0.95 (  0.87)\n",
      "[ 600/2000]\tLoss 1.4271e-01 (1.6313e-01)\tAcc   0.90 (  0.89)\n",
      "[ 800/2000]\tLoss 1.2128e-01 (1.6357e-01)\tAcc   0.90 (  0.89)\n",
      "[1000/2000]\tLoss 3.4883e-01 (1.6368e-01)\tAcc   0.80 (  0.89)\n",
      "[1200/2000]\tLoss 1.6442e-01 (1.5893e-01)\tAcc   0.95 (  0.89)\n",
      "[1400/2000]\tLoss 1.7746e-01 (1.5452e-01)\tAcc   0.85 (  0.90)\n",
      "[1600/2000]\tLoss 1.8870e-01 (1.5311e-01)\tAcc   0.95 (  0.90)\n",
      "[1800/2000]\tLoss 1.7265e-01 (1.5073e-01)\tAcc   0.90 (  0.90)\n",
      "Val loss:0.1659248338639736, Val acc:0.882\n",
      "epoch:  2\n",
      "[   0/2000]\tLoss 1.8780e-01 (1.8780e-01)\tAcc   0.95 (  0.95)\n",
      "[ 200/2000]\tLoss 2.4773e-01 (1.4782e-01)\tAcc   0.75 (  0.89)\n",
      "[ 400/2000]\tLoss 1.0104e-01 (1.3793e-01)\tAcc   1.00 (  0.90)\n",
      "[ 600/2000]\tLoss 1.1520e-01 (1.2770e-01)\tAcc   0.90 (  0.90)\n",
      "[ 800/2000]\tLoss 7.8144e-02 (1.2763e-01)\tAcc   1.00 (  0.91)\n",
      "[1000/2000]\tLoss 2.6504e-01 (1.2623e-01)\tAcc   0.85 (  0.91)\n",
      "[1200/2000]\tLoss 1.2484e-01 (1.2243e-01)\tAcc   0.95 (  0.91)\n",
      "[1400/2000]\tLoss 1.4979e-01 (1.1882e-01)\tAcc   0.85 (  0.91)\n",
      "[1600/2000]\tLoss 1.4898e-01 (1.1822e-01)\tAcc   0.95 (  0.91)\n",
      "[1800/2000]\tLoss 1.4759e-01 (1.1669e-01)\tAcc   0.90 (  0.92)\n",
      "Val loss:0.15286491211503744, Val acc:0.893\n",
      "epoch:  3\n",
      "[   0/2000]\tLoss 1.5754e-01 (1.5754e-01)\tAcc   0.95 (  0.95)\n",
      "[ 200/2000]\tLoss 2.0300e-01 (1.2467e-01)\tAcc   0.80 (  0.90)\n",
      "[ 400/2000]\tLoss 7.9401e-02 (1.1386e-01)\tAcc   1.00 (  0.91)\n",
      "[ 600/2000]\tLoss 9.6419e-02 (1.0419e-01)\tAcc   0.90 (  0.92)\n",
      "[ 800/2000]\tLoss 5.5229e-02 (1.0367e-01)\tAcc   1.00 (  0.93)\n",
      "[1000/2000]\tLoss 2.0393e-01 (1.0153e-01)\tAcc   0.85 (  0.92)\n",
      "[1200/2000]\tLoss 9.2961e-02 (9.8245e-02)\tAcc   0.95 (  0.93)\n",
      "[1400/2000]\tLoss 1.2951e-01 (9.5206e-02)\tAcc   0.85 (  0.93)\n",
      "[1600/2000]\tLoss 1.2154e-01 (9.4974e-02)\tAcc   0.95 (  0.93)\n",
      "[1800/2000]\tLoss 1.2398e-01 (9.3809e-02)\tAcc   0.95 (  0.93)\n",
      "Val loss:0.14589734155684708, Val acc:0.9\n",
      "epoch:  4\n",
      "[   0/2000]\tLoss 1.2691e-01 (1.2691e-01)\tAcc   0.95 (  0.95)\n",
      "[ 200/2000]\tLoss 1.6502e-01 (1.0472e-01)\tAcc   0.80 (  0.91)\n",
      "[ 400/2000]\tLoss 6.5586e-02 (9.4620e-02)\tAcc   1.00 (  0.93)\n",
      "[ 600/2000]\tLoss 8.1384e-02 (8.5985e-02)\tAcc   0.90 (  0.94)\n",
      "[ 800/2000]\tLoss 4.1252e-02 (8.5284e-02)\tAcc   1.00 (  0.94)\n",
      "[1000/2000]\tLoss 1.6206e-01 (8.3029e-02)\tAcc   0.85 (  0.94)\n",
      "[1200/2000]\tLoss 7.0036e-02 (8.0205e-02)\tAcc   0.95 (  0.94)\n",
      "[1400/2000]\tLoss 1.1322e-01 (7.7704e-02)\tAcc   0.85 (  0.94)\n",
      "[1600/2000]\tLoss 1.0069e-01 (7.7679e-02)\tAcc   0.95 (  0.94)\n",
      "[1800/2000]\tLoss 1.0397e-01 (7.6724e-02)\tAcc   0.95 (  0.94)\n",
      "Val loss:0.14206853810697795, Val acc:0.904\n",
      "epoch:  5\n",
      "[   0/2000]\tLoss 9.9547e-02 (9.9547e-02)\tAcc   0.95 (  0.95)\n",
      "[ 200/2000]\tLoss 1.3297e-01 (8.7423e-02)\tAcc   0.80 (  0.91)\n",
      "[ 400/2000]\tLoss 5.5521e-02 (7.8724e-02)\tAcc   1.00 (  0.94)\n",
      "[ 600/2000]\tLoss 6.8443e-02 (7.1313e-02)\tAcc   0.90 (  0.94)\n",
      "[ 800/2000]\tLoss 3.1925e-02 (7.0440e-02)\tAcc   1.00 (  0.95)\n",
      "[1000/2000]\tLoss 1.3213e-01 (6.8400e-02)\tAcc   0.90 (  0.95)\n",
      "[1200/2000]\tLoss 5.4741e-02 (6.6030e-02)\tAcc   0.95 (  0.95)\n",
      "[1400/2000]\tLoss 9.8657e-02 (6.3983e-02)\tAcc   0.85 (  0.95)\n",
      "[1600/2000]\tLoss 8.4756e-02 (6.4101e-02)\tAcc   0.95 (  0.95)\n",
      "[1800/2000]\tLoss 8.7388e-02 (6.3290e-02)\tAcc   0.95 (  0.95)\n",
      "Val loss:0.14024854857474567, Val acc:0.908\n",
      "epoch:  6\n",
      "[   0/2000]\tLoss 7.7791e-02 (7.7791e-02)\tAcc   0.95 (  0.95)\n",
      "[ 200/2000]\tLoss 1.0817e-01 (7.2911e-02)\tAcc   0.80 (  0.92)\n",
      "[ 400/2000]\tLoss 4.7456e-02 (6.5646e-02)\tAcc   1.00 (  0.95)\n",
      "[ 600/2000]\tLoss 5.7410e-02 (5.9392e-02)\tAcc   0.90 (  0.95)\n",
      "[ 800/2000]\tLoss 2.5317e-02 (5.8497e-02)\tAcc   1.00 (  0.95)\n",
      "[1000/2000]\tLoss 1.0850e-01 (5.6753e-02)\tAcc   0.90 (  0.95)\n",
      "[1200/2000]\tLoss 4.3899e-02 (5.4759e-02)\tAcc   0.95 (  0.96)\n",
      "[1400/2000]\tLoss 8.5750e-02 (5.3088e-02)\tAcc   0.90 (  0.96)\n",
      "[1600/2000]\tLoss 7.1473e-02 (5.3283e-02)\tAcc   0.95 (  0.96)\n",
      "[1800/2000]\tLoss 7.3665e-02 (5.2582e-02)\tAcc   0.95 (  0.96)\n",
      "Val loss:0.13971533607691528, Val acc:0.912\n",
      "epoch:  7\n",
      "[   0/2000]\tLoss 6.1366e-02 (6.1366e-02)\tAcc   0.95 (  0.95)\n",
      "[ 200/2000]\tLoss 8.8781e-02 (6.0904e-02)\tAcc   0.85 (  0.93)\n",
      "[ 400/2000]\tLoss 4.0824e-02 (5.4937e-02)\tAcc   1.00 (  0.95)\n",
      "[ 600/2000]\tLoss 4.7843e-02 (4.9676e-02)\tAcc   0.90 (  0.96)\n",
      "[ 800/2000]\tLoss 2.0465e-02 (4.8804e-02)\tAcc   1.00 (  0.96)\n",
      "[1000/2000]\tLoss 8.9973e-02 (4.7380e-02)\tAcc   0.90 (  0.96)\n",
      "[1200/2000]\tLoss 3.5942e-02 (4.5690e-02)\tAcc   1.00 (  0.97)\n",
      "[1400/2000]\tLoss 7.4179e-02 (4.4304e-02)\tAcc   0.90 (  0.97)\n",
      "[1600/2000]\tLoss 6.0681e-02 (4.4543e-02)\tAcc   1.00 (  0.97)\n",
      "[1800/2000]\tLoss 6.2425e-02 (4.3946e-02)\tAcc   0.95 (  0.97)\n",
      "Val loss:0.13995228458195924, Val acc:0.915\n",
      "epoch:  8\n",
      "[   0/2000]\tLoss 4.8830e-02 (4.8830e-02)\tAcc   0.95 (  0.95)\n",
      "[ 200/2000]\tLoss 7.3655e-02 (5.1107e-02)\tAcc   1.00 (  0.96)\n",
      "[ 400/2000]\tLoss 3.5063e-02 (4.6157e-02)\tAcc   1.00 (  0.97)\n",
      "[ 600/2000]\tLoss 4.0113e-02 (4.1767e-02)\tAcc   0.90 (  0.97)\n",
      "[ 800/2000]\tLoss 1.6900e-02 (4.0972e-02)\tAcc   1.00 (  0.97)\n",
      "[1000/2000]\tLoss 7.5154e-02 (3.9828e-02)\tAcc   0.90 (  0.97)\n",
      "[1200/2000]\tLoss 2.9962e-02 (3.8408e-02)\tAcc   1.00 (  0.97)\n",
      "[1400/2000]\tLoss 6.4068e-02 (3.7247e-02)\tAcc   0.95 (  0.98)\n",
      "[1600/2000]\tLoss 5.1957e-02 (3.7505e-02)\tAcc   1.00 (  0.98)\n",
      "[1800/2000]\tLoss 5.3024e-02 (3.6994e-02)\tAcc   0.95 (  0.98)\n",
      "Val loss:0.14080058950930835, Val acc:0.916\n",
      "epoch:  9\n",
      "[   0/2000]\tLoss 3.9603e-02 (3.9603e-02)\tAcc   0.95 (  0.95)\n",
      "[ 200/2000]\tLoss 6.2008e-02 (4.3086e-02)\tAcc   1.00 (  0.97)\n",
      "[ 400/2000]\tLoss 3.0269e-02 (3.9052e-02)\tAcc   1.00 (  0.98)\n",
      "[ 600/2000]\tLoss 3.3583e-02 (3.5361e-02)\tAcc   0.95 (  0.98)\n",
      "[ 800/2000]\tLoss 1.4059e-02 (3.4687e-02)\tAcc   1.00 (  0.98)\n",
      "[1000/2000]\tLoss 6.3258e-02 (3.3786e-02)\tAcc   0.90 (  0.98)\n",
      "[1200/2000]\tLoss 2.5326e-02 (3.2587e-02)\tAcc   1.00 (  0.98)\n",
      "[1400/2000]\tLoss 5.5635e-02 (3.1612e-02)\tAcc   0.95 (  0.98)\n",
      "[1600/2000]\tLoss 4.4529e-02 (3.1864e-02)\tAcc   1.00 (  0.98)\n",
      "[1800/2000]\tLoss 4.5508e-02 (3.1431e-02)\tAcc   0.95 (  0.98)\n",
      "Val loss:0.1420330836623907, Val acc:0.916\n",
      "epoch:  10\n",
      "[   0/2000]\tLoss 3.2763e-02 (3.2763e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 5.2896e-02 (3.6746e-02)\tAcc   1.00 (  0.98)\n",
      "[ 400/2000]\tLoss 2.6146e-02 (3.3414e-02)\tAcc   1.00 (  0.98)\n",
      "[ 600/2000]\tLoss 2.8297e-02 (3.0265e-02)\tAcc   0.95 (  0.98)\n",
      "[ 800/2000]\tLoss 1.1889e-02 (2.9675e-02)\tAcc   1.00 (  0.98)\n",
      "[1000/2000]\tLoss 5.3762e-02 (2.8959e-02)\tAcc   0.90 (  0.98)\n",
      "[1200/2000]\tLoss 2.1729e-02 (2.7929e-02)\tAcc   1.00 (  0.98)\n",
      "[1400/2000]\tLoss 4.8521e-02 (2.7105e-02)\tAcc   0.95 (  0.98)\n",
      "[1600/2000]\tLoss 3.8418e-02 (2.7343e-02)\tAcc   1.00 (  0.98)\n",
      "[1800/2000]\tLoss 3.9340e-02 (2.6975e-02)\tAcc   0.95 (  0.98)\n",
      "Val loss:0.1434042836818844, Val acc:0.915\n",
      "epoch:  11\n",
      "[   0/2000]\tLoss 2.7891e-02 (2.7891e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 4.5774e-02 (3.1729e-02)\tAcc   1.00 (  0.99)\n",
      "[ 400/2000]\tLoss 2.2716e-02 (2.8901e-02)\tAcc   1.00 (  0.99)\n",
      "[ 600/2000]\tLoss 2.4267e-02 (2.6186e-02)\tAcc   1.00 (  0.99)\n",
      "[ 800/2000]\tLoss 1.0225e-02 (2.5678e-02)\tAcc   1.00 (  0.99)\n",
      "[1000/2000]\tLoss 4.6132e-02 (2.5102e-02)\tAcc   0.90 (  0.99)\n",
      "[1200/2000]\tLoss 1.8900e-02 (2.4210e-02)\tAcc   1.00 (  0.99)\n",
      "[1400/2000]\tLoss 4.2363e-02 (2.3497e-02)\tAcc   0.95 (  0.99)\n",
      "[1600/2000]\tLoss 3.3558e-02 (2.3717e-02)\tAcc   1.00 (  0.99)\n",
      "[1800/2000]\tLoss 3.4379e-02 (2.3405e-02)\tAcc   1.00 (  0.99)\n",
      "Val loss:0.1447380391228944, Val acc:0.916\n",
      "epoch:  12\n",
      "[   0/2000]\tLoss 2.4062e-02 (2.4062e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 4.0050e-02 (2.7673e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 1.9851e-02 (2.5277e-02)\tAcc   1.00 (  0.99)\n",
      "[ 600/2000]\tLoss 2.1090e-02 (2.2913e-02)\tAcc   1.00 (  0.99)\n",
      "[ 800/2000]\tLoss 8.9111e-03 (2.2483e-02)\tAcc   1.00 (  0.99)\n",
      "[1000/2000]\tLoss 4.0134e-02 (2.2012e-02)\tAcc   0.90 (  0.99)\n",
      "[1200/2000]\tLoss 1.6598e-02 (2.1230e-02)\tAcc   1.00 (  0.99)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1400/2000]\tLoss 3.7436e-02 (2.0613e-02)\tAcc   0.95 (  0.99)\n",
      "[1600/2000]\tLoss 2.9513e-02 (2.0817e-02)\tAcc   1.00 (  0.99)\n",
      "[1800/2000]\tLoss 3.0399e-02 (2.0552e-02)\tAcc   1.00 (  0.99)\n",
      "Val loss:0.1463260889146477, Val acc:0.916\n",
      "epoch:  13\n",
      "[   0/2000]\tLoss 2.1140e-02 (2.1140e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 3.5660e-02 (2.4518e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 1.7436e-02 (2.2414e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 1.8506e-02 (2.0321e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 7.8611e-03 (1.9940e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 3.5455e-02 (1.9549e-02)\tAcc   0.90 (  0.99)\n",
      "[1200/2000]\tLoss 1.4802e-02 (1.8855e-02)\tAcc   1.00 (  0.99)\n",
      "[1400/2000]\tLoss 3.3281e-02 (1.8313e-02)\tAcc   0.95 (  0.99)\n",
      "[1600/2000]\tLoss 2.6266e-02 (1.8504e-02)\tAcc   1.00 (  0.99)\n",
      "[1800/2000]\tLoss 2.7144e-02 (1.8277e-02)\tAcc   1.00 (  0.99)\n",
      "Val loss:0.14777224292047322, Val acc:0.917\n",
      "epoch:  14\n",
      "[   0/2000]\tLoss 1.8845e-02 (1.8845e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 3.2102e-02 (2.1958e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 1.5500e-02 (2.0105e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 1.6565e-02 (1.8231e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 7.0260e-03 (1.7894e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 3.1742e-02 (1.7567e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 1.3283e-02 (1.6946e-02)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 2.9959e-02 (1.6467e-02)\tAcc   0.95 (  1.00)\n",
      "[1600/2000]\tLoss 2.3612e-02 (1.6644e-02)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 2.4493e-02 (1.6445e-02)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.14925290001556277, Val acc:0.917\n",
      "epoch:  15\n",
      "[   0/2000]\tLoss 1.7134e-02 (1.7134e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 2.9313e-02 (1.9902e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 1.3921e-02 (1.8253e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 1.4908e-02 (1.6549e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 6.3644e-03 (1.6246e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 2.8623e-02 (1.5961e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 1.2114e-02 (1.5401e-02)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 2.7255e-02 (1.4973e-02)\tAcc   0.95 (  1.00)\n",
      "[1600/2000]\tLoss 2.1410e-02 (1.5141e-02)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 2.2343e-02 (1.4971e-02)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.15073541381396352, Val acc:0.917\n",
      "epoch:  16\n",
      "[   0/2000]\tLoss 1.5606e-02 (1.5606e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 2.6937e-02 (1.8248e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 1.2689e-02 (1.6744e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 1.3607e-02 (1.5182e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 5.8284e-03 (1.4908e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 2.6214e-02 (1.4662e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 1.1187e-02 (1.4148e-02)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 2.5026e-02 (1.3759e-02)\tAcc   0.95 (  1.00)\n",
      "[1600/2000]\tLoss 1.9771e-02 (1.3920e-02)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 2.0568e-02 (1.3769e-02)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.15232857153750956, Val acc:0.917\n",
      "epoch:  17\n",
      "[   0/2000]\tLoss 1.4358e-02 (1.4358e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 2.5033e-02 (1.6899e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 1.1691e-02 (1.5517e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 1.2623e-02 (1.4068e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 5.3831e-03 (1.3810e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 2.4137e-02 (1.3590e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 1.0394e-02 (1.3120e-02)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 2.3145e-02 (1.2762e-02)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.8269e-02 (1.2914e-02)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.9068e-02 (1.2779e-02)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.15370632166974246, Val acc:0.916\n",
      "epoch:  18\n",
      "[   0/2000]\tLoss 1.3457e-02 (1.3457e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 2.3403e-02 (1.5805e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 1.0797e-02 (1.4506e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 1.1765e-02 (1.3150e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 5.0156e-03 (1.2908e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 2.2401e-02 (1.2704e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 9.7519e-03 (1.2266e-02)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 2.1586e-02 (1.1935e-02)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.7135e-02 (1.2082e-02)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.7841e-02 (1.1961e-02)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.15486476812977343, Val acc:0.917\n",
      "epoch:  19\n",
      "[   0/2000]\tLoss 1.2734e-02 (1.2734e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 2.2140e-02 (1.4874e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 1.0101e-02 (1.3664e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 1.1028e-02 (1.2382e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 4.7151e-03 (1.2155e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 2.1052e-02 (1.1969e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 9.1908e-03 (1.1554e-02)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 2.0278e-02 (1.1245e-02)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.6111e-02 (1.1386e-02)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.6779e-02 (1.1272e-02)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.15627075413241984, Val acc:0.918\n",
      "epoch:  20\n",
      "[   0/2000]\tLoss 1.1958e-02 (1.1958e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 2.0936e-02 (1.4065e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 9.5701e-03 (1.2927e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 1.0453e-02 (1.1713e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 4.4430e-03 (1.1502e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.9927e-02 (1.1329e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 8.7235e-03 (1.0940e-02)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.9285e-02 (1.0651e-02)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.5241e-02 (1.0789e-02)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.5891e-02 (1.0685e-02)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.15738667283207178, Val acc:0.917\n",
      "epoch:  21\n",
      "[   0/2000]\tLoss 1.1426e-02 (1.1426e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 2.0001e-02 (1.3387e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 9.0776e-03 (1.2305e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 9.9459e-03 (1.1145e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 4.2366e-03 (1.0937e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.8847e-02 (1.0776e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 8.2903e-03 (1.0405e-02)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.8333e-02 (1.0131e-02)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.4494e-02 (1.0263e-02)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.5108e-02 (1.0165e-02)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.15860238763969392, Val acc:0.917\n",
      "epoch:  22\n",
      "[   0/2000]\tLoss 1.0842e-02 (1.0842e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 1.9108e-02 (1.2783e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 8.6762e-03 (1.1756e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 9.5072e-03 (1.0647e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 4.0399e-03 (1.0454e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.7906e-02 (1.0297e-02)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 7.9463e-03 (9.9433e-03)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.7547e-02 (9.6840e-03)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.3903e-02 (9.8153e-03)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.4438e-02 (9.7229e-03)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.15978405499830842, Val acc:0.917\n",
      "epoch:  23\n",
      "[   0/2000]\tLoss 1.0417e-02 (1.0417e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 1.8360e-02 (1.2262e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 8.3292e-03 (1.1277e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 9.0830e-03 (1.0207e-02)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 3.8742e-03 (1.0022e-02)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.7208e-02 (9.8739e-03)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 7.6151e-03 (9.5333e-03)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.6870e-02 (9.2861e-03)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.3314e-02 (9.4154e-03)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.3793e-02 (9.3278e-03)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.16095444083679467, Val acc:0.917\n",
      "epoch:  24\n",
      "[   0/2000]\tLoss 1.0047e-02 (1.0047e-02)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 1.7707e-02 (1.1791e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 7.9782e-03 (1.0841e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 8.7239e-03 (9.8123e-03)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 3.7065e-03 (9.6318e-03)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.6431e-02 (9.4908e-03)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 7.3425e-03 (9.1637e-03)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.6252e-02 (8.9267e-03)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.2848e-02 (9.0521e-03)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.3291e-02 (8.9692e-03)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.16195503409020603, Val acc:0.918\n",
      "epoch:  25\n",
      "[   0/2000]\tLoss 9.6412e-03 (9.6412e-03)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 1.7071e-02 (1.1373e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 7.6841e-03 (1.0458e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 8.3931e-03 (9.4600e-03)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 3.5784e-03 (9.2840e-03)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.5872e-02 (9.1501e-03)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 7.0736e-03 (8.8359e-03)\tAcc   1.00 (  1.00)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1400/2000]\tLoss 1.5731e-02 (8.6081e-03)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.2378e-02 (8.7302e-03)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.2790e-02 (8.6500e-03)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.16291151117999106, Val acc:0.918\n",
      "epoch:  26\n",
      "[   0/2000]\tLoss 9.4101e-03 (9.4101e-03)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 1.6526e-02 (1.0995e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 7.4755e-03 (1.0106e-02)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 8.1039e-03 (9.1391e-03)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 3.4524e-03 (8.9721e-03)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.5291e-02 (8.8431e-03)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 6.8392e-03 (8.5394e-03)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.5193e-02 (8.3205e-03)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.2006e-02 (8.4392e-03)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.2295e-02 (8.3600e-03)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.16403192725032567, Val acc:0.918\n",
      "epoch:  27\n",
      "[   0/2000]\tLoss 9.0330e-03 (9.0330e-03)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 1.6035e-02 (1.0638e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 7.2305e-03 (9.7851e-03)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 7.8466e-03 (8.8477e-03)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 3.3367e-03 (8.6821e-03)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.4808e-02 (8.5589e-03)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 6.6352e-03 (8.2660e-03)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.4768e-02 (8.0565e-03)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.1608e-02 (8.1728e-03)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.1944e-02 (8.0973e-03)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.16490389413200318, Val acc:0.917\n",
      "epoch:  28\n",
      "[   0/2000]\tLoss 8.7360e-03 (8.7360e-03)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 1.5558e-02 (1.0319e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 7.0137e-03 (9.4998e-03)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 7.6157e-03 (8.5846e-03)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 3.2141e-03 (8.4267e-03)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.4309e-02 (8.3060e-03)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 6.4251e-03 (8.0211e-03)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.4359e-02 (7.8165e-03)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.1235e-02 (7.9308e-03)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.1582e-02 (7.8565e-03)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.16584616889245807, Val acc:0.917\n",
      "epoch:  29\n",
      "[   0/2000]\tLoss 8.4983e-03 (8.4983e-03)\tAcc   1.00 (  1.00)\n",
      "[ 200/2000]\tLoss 1.5110e-02 (1.0022e-02)\tAcc   1.00 (  1.00)\n",
      "[ 400/2000]\tLoss 6.8224e-03 (9.2224e-03)\tAcc   1.00 (  1.00)\n",
      "[ 600/2000]\tLoss 7.3965e-03 (8.3329e-03)\tAcc   1.00 (  1.00)\n",
      "[ 800/2000]\tLoss 3.1263e-03 (8.1799e-03)\tAcc   1.00 (  1.00)\n",
      "[1000/2000]\tLoss 1.3961e-02 (8.0658e-03)\tAcc   1.00 (  1.00)\n",
      "[1200/2000]\tLoss 6.2692e-03 (7.7921e-03)\tAcc   1.00 (  1.00)\n",
      "[1400/2000]\tLoss 1.3964e-02 (7.5936e-03)\tAcc   1.00 (  1.00)\n",
      "[1600/2000]\tLoss 1.0939e-02 (7.7042e-03)\tAcc   1.00 (  1.00)\n",
      "[1800/2000]\tLoss 1.1265e-02 (7.6329e-03)\tAcc   1.00 (  1.00)\n",
      "Val loss:0.16682080389000475, Val acc:0.917\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acces = []\n",
    "val_losses = []\n",
    "for epoch in range(30):\n",
    "    print(\"epoch: \", epoch)\n",
    "    losses, acc = train(model, optimizer, device, train_features, train_labels, criteria)\n",
    "    train_loss.append(losses)\n",
    "    train_acc.append(acc)\n",
    "    val_loss, val_acc = evaluate(model, device, val_features, val_labels, criteria)\n",
    "    val_acces.append(val_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Val loss:{}, Val acc:{}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = range(1, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(epoches, train_acc, 'bo', label=\"Training acc\")\n",
    "plt.plot(epoches, val_acces, 'b', label='Validation acc')\n",
    "plt.title(\"Training and Validation acc\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epoches, train_loss, 'bo', label=\"Training loss\")\n",
    "plt.plot(epoches, val_losses, 'b', label='Validation loss')\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit ('pytorch_gpu': conda)",
   "language": "python",
   "name": "python36264bitpytorchgpucondad32ea3e756d94800bb772d25ba7e61ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
